Prompt:
Give me optimized Python code to parse a very large CSV file (500k+ rows) without loading into memory at once. I want it to process in chunks and pass each chunk to SQLAlchemy for bulk upserts by SKU.

AI Output:
- Use Python csv module with streaming file open
- Read file line-by-line, accumulate chunks of 5kâ€“10k
- Use SQLAlchemy bulk_insert_mappings or bulk_update_mappings
- Use lower(sku) for case-insensitive matching
- Recommended pseudocode:
with open(file) as f:
    reader = csv.DictReader(f)
    batch = []
    for row in reader:
        batch.append(transform(row))
        if len(batch) == chunk_size:
            upsert_batch(batch)
            batch.clear()
# final flush
if batch:
    upsert_batch(batch)
